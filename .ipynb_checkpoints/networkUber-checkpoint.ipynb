{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Analysis | Uber \n",
    "## Ray Johnson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My name is Ray Johnson and I am a student finishing my final semester at the Graduate School of Information Science at the University of Illinois in Urbana-Champaign. \n",
    "\n",
    "I end this semester with three courses on Statistical Modeling (with R), an Introduction to Programming with Python, and Network Analysis. In addition, I have had a course in Data Cleaning, Introduction to Databases with MySql, and a Web Design and Accessibility course for Organizations.\n",
    "\n",
    "I've acquired certificates from Lynda.com in Introduction to Programming with JavaScript, and completed three certificates from John Hopkin's Data Science specialization in Data Science.\n",
    "\n",
    "I intend to employ methods I've learned in and out of the university to explore the datasets released to the public with the Freedom of Information Act, in order to see what I can glean from their data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the Data (Data Sources Listed Below, see 'Data' Under 'Bibliography')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information here is paraphrased in part from source listed in Bibliography as \"Uber Pickups in New York City,\" and specific uber drives with context for purpose of drives are cited from \"My Uber Drives Dataset\" listed below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FiveThirtyEight obtained data of 4.5 million Uber pickups in New York City from April to September 2014, then 14.3 additional Uber pickups from January to June 2015. \n",
    "\n",
    "FiveThirtyEight obtained the data from the NYC Taxi and Limousine Commission (TLC) via Freedom of Information Law request on July 20, 2015. \n",
    "\n",
    "- Uber Trip data from 2014 (April-September) separated by month - detailed location info\n",
    "- Uber trip data from 2015 (January-June), less fine-grained location information\n",
    "\n",
    "- Non-Uber For-Hire-Vehicle Trips; while varying by company, the observations might include \"day of trip,\" \"time of trip,\" \"pickup location,\" \"driver's for-hire license number,\" and \"vehicle's for hire license number.\"\n",
    "\n",
    "### Trip Data from 2014, FiveThirtyEight\n",
    "\n",
    "Six files containing raw data of Uber pickups in NYC from April to September 2014, separated by month.\n",
    "\n",
    "Each has the following columns:\n",
    "Date/Time : The date and time of the Uber pickup\n",
    "Lat : The latitude of the Uber pickup\n",
    "Lon : The longitude of the Uber pickup\n",
    "Base : The TLC base company code affiliated with the Uber pickup\n",
    "\n",
    "These files are named:\n",
    "\n",
    "uber-raw-data-apr14.csv\n",
    "uber-raw-data-aug14.csv\n",
    "uber-raw-data-jul14.csv\n",
    "uber-raw-data-jun14.csv\n",
    "uber-raw-data-may14.csv\n",
    "uber-raw-data-sep14.csv\n",
    "Uber trip data from 2015\n",
    "\n",
    "Also included is the file uber-raw-data-janjune-15.csv This file has the following columns:\n",
    "\n",
    "Dispatching_base_num : The TLC base company code of the base that dispatched the Uber\n",
    "Pickup_date : The date and time of the Uber pickup\n",
    "Affiliated_base_num : The TLC base company code affiliated with the Uber pickup\n",
    "locationID : The pickup location ID affiliated with the Uber pickup\n",
    "The Base codes are for the following Uber bases:\n",
    "\n",
    "B02512 : Unter B02598 : Hinter B02617 : Weiter B02682 : Schmecken B02764 : Danach-NY B02765 : Grun B02835 : Dreist B02836 : Drinnen\n",
    "\n",
    "For coarse-grained location information from these pickups, the file taxi-zone-lookup.csv shows the taxi Zone (essentially, neighborhood) and Borough for each locationID.\n",
    "\n",
    "Non-Uber FLV trips\n",
    "\n",
    "The dataset also contains 10 files of raw data on pickups from 10 for-hire vehicle (FHV) companies. The trip information varies by company, but can include day of trip, time of trip, pickup location, driver's for-hire license number, and vehicle's for-hire license number.\n",
    "\n",
    "These files are named:\n",
    "\n",
    "American_B01362.csv\n",
    "Diplo_B01196.csv\n",
    "Highclass_B01717.csv\n",
    "Skyline_B00111.csv\n",
    "Carmel_B00256.csv\n",
    "Federal_02216.csv\n",
    "Lyft_B02510.csv\n",
    "Dial7_B00887.csv\n",
    "Firstclass_B01536.csv\n",
    "Prestige_B01338.csv\n",
    "Aggregate Statistics\n",
    "\n",
    "There is also a file other-FHV-data-jan-aug-2015.csv containing daily pickup data for 329 FHV companies from January 2015 through August 2015.\n",
    "\n",
    "The file Uber-Jan-Feb-FOIL.csv contains aggregated daily Uber trip statistics in January and February 2015.\n",
    "\n",
    "## My Uber Drives (2016) | Dataset of Uber Driver Zeeshan-ul-hassan Usmani\n",
    "### Content\n",
    "Geography: USA, Sri Lanka and Pakistan\n",
    "\n",
    "Time period: January - December 2016\n",
    "\n",
    "Unit of analysis: Drives\n",
    "\n",
    "Total Drives: 1,155\n",
    "\n",
    "Total Miles: 12,204\n",
    "\n",
    "Dataset: The dataset contains Start Date, End Date, Start Location, End Location, Miles Driven and Purpose of drive (Business, Personal, Meals, Errands, Meetings, Customer Support etc.)\n",
    "\n",
    "## Uber_NYC_Enriched, Yannis Pappas \n",
    "### Content\n",
    "https://www.kaggle.com/yannisp/uber-pickups-enriched\n",
    "CSV file of Uber Pickups in NYC from 01/01/15-06/30/15, with weather data from National Centers for Environmental Information, LocationID to Burough mapping by FiveThirtyEight, and NYC public holidays.\n",
    "\n",
    "From YannisPappas' Kaggle Page:\n",
    "- pickup_dt: Time period of the observations.\n",
    "\n",
    "- borough: NYC's borough.\n",
    "\n",
    "- pickups: Number of pickups for the period.\n",
    "\n",
    "- spd: Wind speed in miles/hour.\n",
    "\n",
    "- vsb: Visibility in Miles to nearest tenth.\n",
    "\n",
    "- temp: temperature in Fahrenheit.\n",
    "\n",
    "- dewp: Dew point in Fahrenheit.\n",
    "\n",
    "- slp: Sea level pressure.\n",
    "\n",
    "- pcp01: 1-hour liquid precipitation.\n",
    "\n",
    "- pcp06: 6-hour liquid precipitation.\n",
    "\n",
    "- pcp24: 24-hour liquid precipitation.\n",
    "\n",
    "- sd: Snow depth in inches.\n",
    "\n",
    "- hday: Being a holiday (Y) or not (N)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The work listed below is cited according to the standards of the seventh edition of the Modern Language Association. \n",
    "This aspect of workflow benefitted from Zotero; an auto-generated report of listed sources, including abstracts from works consulted, can be found at this web address: http://cpanel.ischool.illinois.edu/~rrjohns2/bibreport.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Works Consulted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Books:\n",
    "\n",
    "Downey, Allen B. Think Complexity. N.p. Web. 18 Apr. 2017. 1.2.3.\n",
    "\n",
    "### Book Sections\n",
    "Brownley, Clinton W. “Concatenate Data from Multiple Workbooks.” Working with \n",
    "Excel Files in Python. O’Reilly Media, Inc., 2016. Safari Books Online. Web. 8 May 2017.\n",
    "\n",
    "Diener, Michael. “Python Geospatial Analysis Cookbook.” Python Geospatial \n",
    "Analysis Cookbook. Packt Publishing, 2015. Safari Books Online. Web. 8 May 2017.\n",
    "\n",
    "ericmjl. “Network-Analysis-Made-Simple.” GitHub. N.p., n.d. Web. 21 Apr. 2017.\n",
    "\n",
    "Fandango, Armando. “Statistics and Linear Algebra.” Python Data Analysis - Second Edition. 2nd ed. Packt Publishing, 2017. Safari Books Online. Web. 8 May 2017.\n",
    "\n",
    "Idris, Ivan. “Creating Attractive Data Visualizations.” Python Data Analysis \n",
    "Cookbook. Packt Publishing, 2016. Safari Books Online. Web. 8 May 2017.\n",
    "\n",
    "Lawhead, Joel. “Automating QGIS.” QGIS Python Programming Cookbook - Second Edition. 2nd ed. Packt Publishing, 2017. Safari Books Online. Web. 8 May 2017.\n",
    "\n",
    "Madhavan, Samir. “Uncovering Machine Learning.” Mastering Python for Data Science. Packt Publishing, 2015. Safari Books Online. Web. 8 May 2017.\n",
    "\n",
    "McKinney, Wes. “Data Loading, Storage, and File Formats.” Python for Data Analysis. O’Reilly Media, Inc., 2012. Safari Books Online. Web. 8 May 2017.\n",
    "\n",
    "---. “Data Wrangling: Clean, Transform, Merge, Reshape.” Python for Data Analysis. O’Reilly Media, Inc., 2012. Safari Books Online. Web. 8 May 2017.\n",
    "\n",
    "---. “Plotting and Visualization.” Python for Data Analysis. O’Reilly Media, Inc., 2012. Safari Books Online. Web. 8 May 2017.\n",
    "\n",
    "Mueller, John Paul, and Luca Massaron. “Dealing with Dates in Your Data.” Python for Data Science For Dummies. For Dummies, 2015. Safari Books Online. Web. 8 May 2017.\n",
    "\n",
    "---. “Dealing with Missing Data.” Python for Data Science For Dummies. For Dummies, 2015. Safari Books Online. Web. 8 May 2017.\n",
    "\n",
    "---. “Visualizing the Data.” Python for Data Science For Dummies. For Dummies, 2015. Safari Books Online. Web. 8 May 2017.\n",
    "\n",
    "Nelli, Fabio. “Appendix B: Open Data Sources.” Python Data Analytics: Data Analysis and Science Using Pandas, Matplotlib, and the Python Programming Language. Apress, 2015. Safari Books Online. Web. 8 May 2017.\n",
    "\n",
    "---. “Data Visualization with Matplotlib.” Python Data Analytics: Data Analysis and Science Using Pandas, Matplotlib, and the Python Programming Language. Apress, 2015. Safari Books Online. Web. 8 May 2017.\n",
    "\n",
    "---. “Python and Data Analysis.” Python Data Analytics: Data Analysis and Science Using Pandas, Matplotlib, and the Python Programming Language. Apress, 2015. Safari Books Online. Web. 8 May 2017.\n",
    "\n",
    "Persson, Magnus Vilhelm, and Luiz Felipe Martins. “A. More on Jupyter Notebook and Matplotlib Styles.” Mastering Python Data Analysis. Packt Publishing, 2016. Safari Books Online. Web. 8 May 2017.\n",
    "\n",
    "---. “Learning About Models.” Mastering Python Data Analysis. Packt Publishing, 2016. Safari Books Online. Web. 8 May 2017.\n",
    "\n",
    "Squire, Megan. “Expanding Your Data Mining Toolbox.” Mastering Data Mining with Python – Find Patterns Hidden in Your Data. Packt Publishing, 2016. Safari Books Online. Web. 8 May 2017.\n",
    "\n",
    "Subramanian, Gopi. “Data Analysis – Explore and Wrangle.” Python Data Science Cookbook. Packt Publishing, 2015. Safari Books Online. Web. 8 May 2017.\n",
    "Vo.T.H, Phuong et al. “Advanced Uses of Pandas for Data Analysis.” Python: Data Analytics and Visualization. Packt Publishing, 2017. Safari Books Online. Web. 8 May 2017.\n",
    "\n",
    "\n",
    "### DataCamp Course:\n",
    "\n",
    "Ma, Eric. “Introduction to Networks | Python.” Web. 21 Apr. 2017.\n",
    "\n",
    "### GitHub Repository Walk-Throughs:\n",
    "\n",
    "ericmjl. “Network-Analysis-Made-Simple.” GitHub. Web. 18 Apr. 2017. https://github.com/ericmjl/Network-Analysis-Made-Simple.\n",
    "\n",
    "briatte. \"Awesome Network Analysis.\" GitHub. 14 Apr. 2017\n",
    "https://github.com/briatte/awesome-network-analysis.\n",
    "\n",
    "\"Network Analysis Made Simple.\" GitHub. 14 Apr. 2017.\n",
    "https://github.com/ericmjl/Network-Analysis-Made-Simple\n",
    "\n",
    "## Data Resources\n",
    "\n",
    "### Uber Data - Over 4.5 Million Uber pick ups in New York City Explored\n",
    "\n",
    "andrewflowers. “Fivethirtyeight/Uber-Tlc-Foil-Response.” GitHub. Web. 16 Apr. 2017.\n",
    "https://github.com/toddwschneider/nyc-taxi-data.\n",
    "\n",
    "\"Social Network Analysis with Python,\" a talk given by Maksim Tsvetovat and Alex Kouznetsov at PyCon 2012.\n",
    "https://github.com/maksim2042/PyCon2012\n",
    "\n",
    "### \"My Uber Drives\" Data\n",
    "Zeeshan-ul-hassan Usmani, My Uber Drives Dataset, Kaggle Dataset Repository, March 23, 2017.”\n",
    "\n",
    "### \"Uber Pickups in New York City\" Data\n",
    "“Uber Pickups in New York City.” N.p., n.d. Web. 8 May 2017. https://www.kaggle.com/fivethirtyeight/uber-pickups-in-new-york-city\n",
    "\n",
    "### Freedom of Information Act - FOIL Datasets, Uber\n",
    "\n",
    "https://github.com/fivethirtyeight/uber-tlc-foil-response/tree/master/uber-trip-data\n",
    "\n",
    "https://archive.org/details/nycTaxiTripData2013"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Necessary Modules for Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import networkx as nx # 'creation, manipulation, and study of structure, dynamics, and functions of complex networks' \n",
    "import matplotlib.pyplot as plt # plotting library/numerical mathematics extensions\n",
    "import seaborn as sns # attractive/informative stat graphics in Python - built on top of matplotlib\n",
    "import datetime\n",
    "from sklearn.cluster import KMeans # K-Means Clustering\n",
    "from sklearn.decomposition import PCA # Principal Component Analysis Module\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataframe for January-February Uber data. Data is stored on an external hard drive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfJanFeb =pd.read_csv(\"D:/networkUber/pickupsNY/Uber-Jan-Feb-FOIL.csv\") # reads and creates df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    dispatching_base_number       date  active_vehicles  trips\n",
      "0                    B02512   1/1/2015              190   1132\n",
      "1                    B02765   1/1/2015              225   1765\n",
      "2                    B02764   1/1/2015             3427  29421\n",
      "3                    B02682   1/1/2015              945   7679\n",
      "4                    B02617   1/1/2015             1228   9537\n",
      "5                    B02598   1/1/2015              870   6903\n",
      "6                    B02598   1/2/2015              785   4768\n",
      "7                    B02617   1/2/2015             1137   7065\n",
      "8                    B02512   1/2/2015              175    875\n",
      "9                    B02682   1/2/2015              890   5506\n",
      "10                   B02765   1/2/2015              196   1001\n",
      "11                   B02764   1/2/2015             3147  19974\n",
      "12                   B02765   1/3/2015              201   1526\n",
      "13                   B02617   1/3/2015             1188  10664\n",
      "14                   B02598   1/3/2015              818   7432\n",
      "15                   B02682   1/3/2015              915   8010\n",
      "16                   B02512   1/3/2015              173   1088\n",
      "17                   B02764   1/3/2015             3215  29729\n",
      "18                   B02512   1/4/2015              147    791\n",
      "19                   B02682   1/4/2015              812   5621\n",
      "20                   B02598   1/4/2015              746   5223\n",
      "21                   B02765   1/4/2015              183    993\n",
      "22                   B02617   1/4/2015             1088   7729\n",
      "23                   B02764   1/4/2015             2862  20441\n",
      "24                   B02512   1/5/2015              194    984\n",
      "25                   B02682   1/5/2015              951   6012\n",
      "26                   B02617   1/5/2015             1218   7899\n",
      "27                   B02764   1/5/2015             3387  20926\n",
      "28                   B02598   1/5/2015              907   5798\n",
      "29                   B02765   1/5/2015              227   1133\n",
      "..                      ...        ...              ...    ...\n",
      "324                  B02764  2/24/2015             3965  34686\n",
      "325                  B02512  2/24/2015              247   1869\n",
      "326                  B02598  2/24/2015             1061   9954\n",
      "327                  B02682  2/24/2015             1346  12497\n",
      "328                  B02617  2/24/2015             1456  13719\n",
      "329                  B02765  2/24/2015              698   6390\n",
      "330                  B02512  2/25/2015              246   1647\n",
      "331                  B02598  2/25/2015             1076   9405\n",
      "332                  B02765  2/25/2015              706   6178\n",
      "333                  B02682  2/25/2015             1395  12693\n",
      "334                  B02617  2/25/2015             1473  12811\n",
      "335                  B02764  2/25/2015             3934  31957\n",
      "336                  B02598  2/26/2015             1134  10661\n",
      "337                  B02617  2/26/2015             1539  14461\n",
      "338                  B02682  2/26/2015             1465  13814\n",
      "339                  B02512  2/26/2015              243   1797\n",
      "340                  B02765  2/26/2015              745   6744\n",
      "341                  B02764  2/26/2015             4101  36091\n",
      "342                  B02765  2/27/2015              786   7563\n",
      "343                  B02617  2/27/2015             1551  14677\n",
      "344                  B02598  2/27/2015             1114  10755\n",
      "345                  B02512  2/27/2015              272   2056\n",
      "346                  B02764  2/27/2015             4253  38780\n",
      "347                  B02682  2/27/2015             1510  14975\n",
      "348                  B02598  2/28/2015              994  10319\n",
      "349                  B02764  2/28/2015             3952  39812\n",
      "350                  B02617  2/28/2015             1372  14022\n",
      "351                  B02682  2/28/2015             1386  14472\n",
      "352                  B02512  2/28/2015              230   1803\n",
      "353                  B02765  2/28/2015              747   7753\n",
      "\n",
      "[354 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       1/1/2015\n",
      "1       1/1/2015\n",
      "2       1/1/2015\n",
      "3       1/1/2015\n",
      "4       1/1/2015\n",
      "5       1/1/2015\n",
      "6       1/2/2015\n",
      "7       1/2/2015\n",
      "8       1/2/2015\n",
      "9       1/2/2015\n",
      "10      1/2/2015\n",
      "11      1/2/2015\n",
      "12      1/3/2015\n",
      "13      1/3/2015\n",
      "14      1/3/2015\n",
      "15      1/3/2015\n",
      "16      1/3/2015\n",
      "17      1/3/2015\n",
      "18      1/4/2015\n",
      "19      1/4/2015\n",
      "20      1/4/2015\n",
      "21      1/4/2015\n",
      "22      1/4/2015\n",
      "23      1/4/2015\n",
      "24      1/5/2015\n",
      "25      1/5/2015\n",
      "26      1/5/2015\n",
      "27      1/5/2015\n",
      "28      1/5/2015\n",
      "29      1/5/2015\n",
      "         ...    \n",
      "324    2/24/2015\n",
      "325    2/24/2015\n",
      "326    2/24/2015\n",
      "327    2/24/2015\n",
      "328    2/24/2015\n",
      "329    2/24/2015\n",
      "330    2/25/2015\n",
      "331    2/25/2015\n",
      "332    2/25/2015\n",
      "333    2/25/2015\n",
      "334    2/25/2015\n",
      "335    2/25/2015\n",
      "336    2/26/2015\n",
      "337    2/26/2015\n",
      "338    2/26/2015\n",
      "339    2/26/2015\n",
      "340    2/26/2015\n",
      "341    2/26/2015\n",
      "342    2/27/2015\n",
      "343    2/27/2015\n",
      "344    2/27/2015\n",
      "345    2/27/2015\n",
      "346    2/27/2015\n",
      "347    2/27/2015\n",
      "348    2/28/2015\n",
      "349    2/28/2015\n",
      "350    2/28/2015\n",
      "351    2/28/2015\n",
      "352    2/28/2015\n",
      "353    2/28/2015\n",
      "Name: date, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#creating date dataframe\n",
    "datedf=df.loc[:,'date']\n",
    "print(datedf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count how many trips were taken from different base numbers included in data; ignore sorting, because the value is verifiably consistent from each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1/2/2015     6\n",
       "2/28/2015    6\n",
       "2/1/2015     6\n",
       "1/11/2015    6\n",
       "2/7/2015     6\n",
       "1/7/2015     6\n",
       "2/9/2015     6\n",
       "1/13/2015    6\n",
       "1/26/2015    6\n",
       "2/12/2015    6\n",
       "2/26/2015    6\n",
       "2/6/2015     6\n",
       "1/15/2015    6\n",
       "2/4/2015     6\n",
       "1/18/2015    6\n",
       "2/2/2015     6\n",
       "1/27/2015    6\n",
       "2/5/2015     6\n",
       "2/25/2015    6\n",
       "1/28/2015    6\n",
       "2/16/2015    6\n",
       "2/3/2015     6\n",
       "2/11/2015    6\n",
       "2/18/2015    6\n",
       "1/23/2015    6\n",
       "1/6/2015     6\n",
       "1/8/2015     6\n",
       "1/25/2015    6\n",
       "1/12/2015    6\n",
       "1/31/2015    6\n",
       "2/20/2015    6\n",
       "2/23/2015    6\n",
       "1/30/2015    6\n",
       "1/5/2015     6\n",
       "1/3/2015     6\n",
       "1/10/2015    6\n",
       "1/22/2015    6\n",
       "1/19/2015    6\n",
       "2/17/2015    6\n",
       "1/14/2015    6\n",
       "1/20/2015    6\n",
       "2/14/2015    6\n",
       "1/1/2015     6\n",
       "2/10/2015    6\n",
       "2/27/2015    6\n",
       "1/29/2015    6\n",
       "2/15/2015    6\n",
       "2/24/2015    6\n",
       "2/19/2015    6\n",
       "1/24/2015    6\n",
       "1/9/2015     6\n",
       "2/22/2015    6\n",
       "1/17/2015    6\n",
       "2/8/2015     6\n",
       "1/21/2015    6\n",
       "1/4/2015     6\n",
       "2/21/2015    6\n",
       "1/16/2015    6\n",
       "2/13/2015    6\n",
       "Name: date, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datedf.value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uberApril2014 = pd.read_csv(\"D:/networkUber/pickupsNY/uber-raw-data-apr14.csv\", index_col = False)\n",
    "uberApril2014.columns=['Date', 'Latitude', 'Longitude', 'Base']\n",
    "uberApril2014.drop(['Base'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uberApril2014[\"Type\"]=\"Uber\"\n",
    "uberApril2014[\"Date\"]=pd.to_datetime(uberApril2014[\"Date\"]).dt.strftime('%Y-%m-%d-%H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uberJanJune15df = pd.read_csv(\"D:/networkUber/pickupsNY/uber-raw-data-janjune-15.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Dispatching_base_num          Pickup_date Affiliated_base_num  \\\n",
      "0                      B02617  2015-05-17 09:47:00              B02617   \n",
      "1                      B02617  2015-05-17 09:47:00              B02617   \n",
      "2                      B02617  2015-05-17 09:47:00              B02617   \n",
      "3                      B02617  2015-05-17 09:47:00              B02774   \n",
      "4                      B02617  2015-05-17 09:47:00              B02617   \n",
      "5                      B02617  2015-05-17 09:47:00              B02617   \n",
      "6                      B02617  2015-05-17 09:47:00              B02617   \n",
      "7                      B02617  2015-05-17 09:47:00              B02764   \n",
      "8                      B02617  2015-05-17 09:47:00              B02617   \n",
      "9                      B02617  2015-05-17 09:47:00              B02764   \n",
      "10                     B02617  2015-05-17 09:48:00              B02617   \n",
      "11                     B02617  2015-05-17 09:48:00              B02617   \n",
      "12                     B02617  2015-05-17 09:49:00              B02617   \n",
      "13                     B02617  2015-05-17 09:49:00              B02617   \n",
      "14                     B02617  2015-05-17 09:49:00              B02617   \n",
      "15                     B02617  2015-05-17 09:49:00              B02617   \n",
      "16                     B02617  2015-05-17 09:49:00              B02617   \n",
      "17                     B02617  2015-05-17 09:49:00              B02617   \n",
      "18                     B02617  2015-05-17 09:49:00              B02617   \n",
      "19                     B02617  2015-05-17 09:49:00              B02617   \n",
      "20                     B02617  2015-05-17 09:49:00              B02682   \n",
      "21                     B02617  2015-05-17 09:50:00              B02617   \n",
      "22                     B02617  2015-05-17 09:50:00              B02617   \n",
      "23                     B02617  2015-05-17 09:50:00              B02617   \n",
      "24                     B02617  2015-05-17 09:50:00              B02617   \n",
      "25                     B02617  2015-05-17 09:50:00              B02617   \n",
      "26                     B02617  2015-05-17 09:50:00              B02617   \n",
      "27                     B02617  2015-05-17 09:50:00              B02764   \n",
      "28                     B02617  2015-05-17 09:50:00              B02617   \n",
      "29                     B02617  2015-05-17 09:51:00              B02617   \n",
      "...                       ...                  ...                 ...   \n",
      "14270449               B02765  2015-05-08 15:39:00              B02765   \n",
      "14270450               B02765  2015-05-08 15:39:00              B00014   \n",
      "14270451               B02765  2015-05-08 15:39:00              B02765   \n",
      "14270452               B02765  2015-05-08 15:39:00              B02765   \n",
      "14270453               B02765  2015-05-08 15:39:00              B02765   \n",
      "14270454               B02765  2015-05-08 15:39:00              B02682   \n",
      "14270455               B02765  2015-05-08 15:40:00              B00271   \n",
      "14270456               B02765  2015-05-08 15:40:00              B02765   \n",
      "14270457               B02765  2015-05-08 15:40:00              B00789   \n",
      "14270458               B02765  2015-05-08 15:40:00              B02765   \n",
      "14270459               B02765  2015-05-08 15:40:00              B02765   \n",
      "14270460               B02765  2015-05-08 15:41:00              B02764   \n",
      "14270461               B02765  2015-05-08 15:41:00              B02253   \n",
      "14270462               B02765  2015-05-08 15:41:00              B02682   \n",
      "14270463               B02765  2015-05-08 15:41:00              B02345   \n",
      "14270464               B02765  2015-05-08 15:41:00              B02765   \n",
      "14270465               B02765  2015-05-08 15:41:00              B02765   \n",
      "14270466               B02765  2015-05-08 15:41:00              B02765   \n",
      "14270467               B02765  2015-05-08 15:41:00              B02765   \n",
      "14270468               B02765  2015-05-08 15:42:00              B02096   \n",
      "14270469               B02765  2015-05-08 15:42:00              B02764   \n",
      "14270470               B02765  2015-05-08 15:42:00              B02765   \n",
      "14270471               B02765  2015-05-08 15:42:00              B02765   \n",
      "14270472               B02765  2015-05-08 15:42:00              B02765   \n",
      "14270473               B02765  2015-05-08 15:43:00              B02711   \n",
      "14270474               B02765  2015-05-08 15:43:00              B02765   \n",
      "14270475               B02765  2015-05-08 15:43:00              B02765   \n",
      "14270476               B02765  2015-05-08 15:43:00              B02765   \n",
      "14270477               B02765  2015-05-08 15:44:00              B01899   \n",
      "14270478               B02765  2015-05-08 15:44:00              B02682   \n",
      "\n",
      "          locationID  \n",
      "0                141  \n",
      "1                 65  \n",
      "2                100  \n",
      "3                 80  \n",
      "4                 90  \n",
      "5                228  \n",
      "6                  7  \n",
      "7                 74  \n",
      "8                249  \n",
      "9                 22  \n",
      "10               263  \n",
      "11                61  \n",
      "12               229  \n",
      "13               164  \n",
      "14               237  \n",
      "15               142  \n",
      "16               188  \n",
      "17               237  \n",
      "18               224  \n",
      "19               238  \n",
      "20               242  \n",
      "21                95  \n",
      "22               141  \n",
      "23               236  \n",
      "24               233  \n",
      "25               230  \n",
      "26               162  \n",
      "27               234  \n",
      "28               161  \n",
      "29                87  \n",
      "...              ...  \n",
      "14270449         163  \n",
      "14270450         239  \n",
      "14270451         239  \n",
      "14270452         238  \n",
      "14270453         211  \n",
      "14270454          68  \n",
      "14270455         170  \n",
      "14270456         211  \n",
      "14270457         236  \n",
      "14270458         161  \n",
      "14270459         186  \n",
      "14270460          50  \n",
      "14270461          13  \n",
      "14270462         192  \n",
      "14270463         262  \n",
      "14270464         237  \n",
      "14270465         233  \n",
      "14270466         148  \n",
      "14270467          33  \n",
      "14270468         232  \n",
      "14270469          79  \n",
      "14270470          37  \n",
      "14270471         161  \n",
      "14270472           7  \n",
      "14270473          25  \n",
      "14270474         186  \n",
      "14270475         263  \n",
      "14270476          90  \n",
      "14270477          45  \n",
      "14270478         144  \n",
      "\n",
      "[14270479 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(uberJanJune15df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Modelling Uber Data with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enhancedJanJune15 = pd.read_csv(\"D:/networkUber/uber_nyc_enriched.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 pickup_dt        borough  pickups  spd   vsb  temp  dewp  \\\n",
      "0      2015-01-01 01:00:00          Bronx      152  5.0  10.0  30.0   7.0   \n",
      "1      2015-01-01 01:00:00       Brooklyn     1519  5.0  10.0  30.0   7.0   \n",
      "2      2015-01-01 01:00:00            EWR        0  5.0  10.0  30.0   7.0   \n",
      "3      2015-01-01 01:00:00      Manhattan     5258  5.0  10.0  30.0   7.0   \n",
      "4      2015-01-01 01:00:00         Queens      405  5.0  10.0  30.0   7.0   \n",
      "5      2015-01-01 01:00:00  Staten Island        6  5.0  10.0  30.0   7.0   \n",
      "6      2015-01-01 01:00:00            NaN        4  5.0  10.0  30.0   7.0   \n",
      "7      2015-01-01 02:00:00          Bronx      120  3.0  10.0  30.0   6.0   \n",
      "8      2015-01-01 02:00:00       Brooklyn     1229  3.0  10.0  30.0   6.0   \n",
      "9      2015-01-01 02:00:00            EWR        0  3.0  10.0  30.0   6.0   \n",
      "10     2015-01-01 02:00:00      Manhattan     4345  3.0  10.0  30.0   6.0   \n",
      "11     2015-01-01 02:00:00         Queens      331  3.0  10.0  30.0   6.0   \n",
      "12     2015-01-01 02:00:00  Staten Island        7  3.0  10.0  30.0   6.0   \n",
      "13     2015-01-01 02:00:00            NaN       11  3.0  10.0  30.0   6.0   \n",
      "14     2015-01-01 03:00:00          Bronx      132  5.0  10.0  30.0   8.0   \n",
      "15     2015-01-01 03:00:00       Brooklyn     1601  5.0  10.0  30.0   8.0   \n",
      "16     2015-01-01 03:00:00            EWR        0  5.0  10.0  30.0   8.0   \n",
      "17     2015-01-01 03:00:00      Manhattan     4577  5.0  10.0  30.0   8.0   \n",
      "18     2015-01-01 03:00:00         Queens      440  5.0  10.0  30.0   8.0   \n",
      "19     2015-01-01 03:00:00  Staten Island       12  5.0  10.0  30.0   8.0   \n",
      "20     2015-01-01 03:00:00            NaN        1  5.0  10.0  30.0   8.0   \n",
      "21     2015-01-01 04:00:00          Bronx      128  5.0  10.0  29.0   9.0   \n",
      "22     2015-01-01 04:00:00       Brooklyn     1390  5.0  10.0  29.0   9.0   \n",
      "23     2015-01-01 04:00:00            EWR        0  5.0  10.0  29.0   9.0   \n",
      "24     2015-01-01 04:00:00      Manhattan     3003  5.0  10.0  29.0   9.0   \n",
      "25     2015-01-01 04:00:00         Queens      344  5.0  10.0  29.0   9.0   \n",
      "26     2015-01-01 04:00:00  Staten Island        5  5.0  10.0  29.0   9.0   \n",
      "27     2015-01-01 04:00:00            NaN        2  5.0  10.0  29.0   9.0   \n",
      "28     2015-01-01 05:00:00          Bronx       87  5.0  10.0  28.0   9.0   \n",
      "29     2015-01-01 05:00:00       Brooklyn      759  5.0  10.0  28.0   9.0   \n",
      "...                    ...            ...      ...  ...   ...   ...   ...   \n",
      "29071  2015-06-30 19:00:00      Manhattan     4585  5.0  10.0  80.0  58.0   \n",
      "29072  2015-06-30 19:00:00         Queens      417  5.0  10.0  80.0  58.0   \n",
      "29073  2015-06-30 19:00:00  Staten Island        3  5.0  10.0  80.0  58.0   \n",
      "29074  2015-06-30 19:00:00            NaN        4  5.0  10.0  80.0  58.0   \n",
      "29075  2015-06-30 20:00:00          Bronx       66  8.0  10.0  80.0  58.0   \n",
      "29076  2015-06-30 20:00:00       Brooklyn      817  8.0  10.0  80.0  58.0   \n",
      "29077  2015-06-30 20:00:00            EWR        0  8.0  10.0  80.0  58.0   \n",
      "29078  2015-06-30 20:00:00      Manhattan     4210  8.0  10.0  80.0  58.0   \n",
      "29079  2015-06-30 20:00:00         Queens      438  8.0  10.0  80.0  58.0   \n",
      "29080  2015-06-30 20:00:00  Staten Island        3  8.0  10.0  80.0  58.0   \n",
      "29081  2015-06-30 20:00:00            NaN        5  8.0  10.0  80.0  58.0   \n",
      "29082  2015-06-30 21:00:00          Bronx       57  7.0  10.0  76.0  61.0   \n",
      "29083  2015-06-30 21:00:00       Brooklyn      829  7.0  10.0  76.0  61.0   \n",
      "29084  2015-06-30 21:00:00            EWR        0  7.0  10.0  76.0  61.0   \n",
      "29085  2015-06-30 21:00:00      Manhattan     4313  7.0  10.0  76.0  61.0   \n",
      "29086  2015-06-30 21:00:00         Queens      483  7.0  10.0  76.0  61.0   \n",
      "29087  2015-06-30 21:00:00  Staten Island        2  7.0  10.0  76.0  61.0   \n",
      "29088  2015-06-30 22:00:00          Bronx       53  5.0  10.0  76.0  64.0   \n",
      "29089  2015-06-30 22:00:00       Brooklyn      860  5.0  10.0  76.0  64.0   \n",
      "29090  2015-06-30 22:00:00            EWR        0  5.0  10.0  76.0  64.0   \n",
      "29091  2015-06-30 22:00:00      Manhattan     4452  5.0  10.0  76.0  64.0   \n",
      "29092  2015-06-30 22:00:00         Queens      556  5.0  10.0  76.0  64.0   \n",
      "29093  2015-06-30 22:00:00  Staten Island        2  5.0  10.0  76.0  64.0   \n",
      "29094  2015-06-30 23:00:00          Bronx       67  7.0  10.0  75.0  65.0   \n",
      "29095  2015-06-30 23:00:00       Brooklyn      990  7.0  10.0  75.0  65.0   \n",
      "29096  2015-06-30 23:00:00            EWR        0  7.0  10.0  75.0  65.0   \n",
      "29097  2015-06-30 23:00:00      Manhattan     3828  7.0  10.0  75.0  65.0   \n",
      "29098  2015-06-30 23:00:00         Queens      580  7.0  10.0  75.0  65.0   \n",
      "29099  2015-06-30 23:00:00  Staten Island        0  7.0  10.0  75.0  65.0   \n",
      "29100  2015-06-30 23:00:00            NaN        3  7.0  10.0  75.0  65.0   \n",
      "\n",
      "          slp  pcp01  pcp06  pcp24   sd hday  \n",
      "0      1023.5    0.0    0.0    0.0  0.0    Y  \n",
      "1      1023.5    0.0    0.0    0.0  0.0    Y  \n",
      "2      1023.5    0.0    0.0    0.0  0.0    Y  \n",
      "3      1023.5    0.0    0.0    0.0  0.0    Y  \n",
      "4      1023.5    0.0    0.0    0.0  0.0    Y  \n",
      "5      1023.5    0.0    0.0    0.0  0.0    Y  \n",
      "6      1023.5    0.0    0.0    0.0  0.0    Y  \n",
      "7      1023.0    0.0    0.0    0.0  0.0    Y  \n",
      "8      1023.0    0.0    0.0    0.0  0.0    Y  \n",
      "9      1023.0    0.0    0.0    0.0  0.0    Y  \n",
      "10     1023.0    0.0    0.0    0.0  0.0    Y  \n",
      "11     1023.0    0.0    0.0    0.0  0.0    Y  \n",
      "12     1023.0    0.0    0.0    0.0  0.0    Y  \n",
      "13     1023.0    0.0    0.0    0.0  0.0    Y  \n",
      "14     1022.3    0.0    0.0    0.0  0.0    Y  \n",
      "15     1022.3    0.0    0.0    0.0  0.0    Y  \n",
      "16     1022.3    0.0    0.0    0.0  0.0    Y  \n",
      "17     1022.3    0.0    0.0    0.0  0.0    Y  \n",
      "18     1022.3    0.0    0.0    0.0  0.0    Y  \n",
      "19     1022.3    0.0    0.0    0.0  0.0    Y  \n",
      "20     1022.3    0.0    0.0    0.0  0.0    Y  \n",
      "21     1022.0    0.0    0.0    0.0  0.0    Y  \n",
      "22     1022.0    0.0    0.0    0.0  0.0    Y  \n",
      "23     1022.0    0.0    0.0    0.0  0.0    Y  \n",
      "24     1022.0    0.0    0.0    0.0  0.0    Y  \n",
      "25     1022.0    0.0    0.0    0.0  0.0    Y  \n",
      "26     1022.0    0.0    0.0    0.0  0.0    Y  \n",
      "27     1022.0    0.0    0.0    0.0  0.0    Y  \n",
      "28     1021.8    0.0    0.0    0.0  0.0    Y  \n",
      "29     1021.8    0.0    0.0    0.0  0.0    Y  \n",
      "...       ...    ...    ...    ...  ...  ...  \n",
      "29071  1012.5    0.0    0.0    0.0  0.0    N  \n",
      "29072  1012.5    0.0    0.0    0.0  0.0    N  \n",
      "29073  1012.5    0.0    0.0    0.0  0.0    N  \n",
      "29074  1012.5    0.0    0.0    0.0  0.0    N  \n",
      "29075  1012.4    0.0    0.0    0.0  0.0    N  \n",
      "29076  1012.4    0.0    0.0    0.0  0.0    N  \n",
      "29077  1012.4    0.0    0.0    0.0  0.0    N  \n",
      "29078  1012.4    0.0    0.0    0.0  0.0    N  \n",
      "29079  1012.4    0.0    0.0    0.0  0.0    N  \n",
      "29080  1012.4    0.0    0.0    0.0  0.0    N  \n",
      "29081  1012.4    0.0    0.0    0.0  0.0    N  \n",
      "29082  1012.4    0.0    0.0    0.0  0.0    N  \n",
      "29083  1012.4    0.0    0.0    0.0  0.0    N  \n",
      "29084  1012.4    0.0    0.0    0.0  0.0    N  \n",
      "29085  1012.4    0.0    0.0    0.0  0.0    N  \n",
      "29086  1012.4    0.0    0.0    0.0  0.0    N  \n",
      "29087  1012.4    0.0    0.0    0.0  0.0    N  \n",
      "29088  1011.9    0.0    0.0    0.0  0.0    N  \n",
      "29089  1011.9    0.0    0.0    0.0  0.0    N  \n",
      "29090  1011.9    0.0    0.0    0.0  0.0    N  \n",
      "29091  1011.9    0.0    0.0    0.0  0.0    N  \n",
      "29092  1011.9    0.0    0.0    0.0  0.0    N  \n",
      "29093  1011.9    0.0    0.0    0.0  0.0    N  \n",
      "29094  1011.8    0.0    0.0    0.0  0.0    N  \n",
      "29095  1011.8    0.0    0.0    0.0  0.0    N  \n",
      "29096  1011.8    0.0    0.0    0.0  0.0    N  \n",
      "29097  1011.8    0.0    0.0    0.0  0.0    N  \n",
      "29098  1011.8    0.0    0.0    0.0  0.0    N  \n",
      "29099  1011.8    0.0    0.0    0.0  0.0    N  \n",
      "29100  1011.8    0.0    0.0    0.0  0.0    N  \n",
      "\n",
      "[29101 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "print(enhancedJanJune15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
